import requests

def generate_image_caption(img_path, api_key):
    return "ì´ë¯¸ì§€ ì„¤ëª… ê¸°ëŠ¥ì€ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

def speech_to_text(audio_path, api_key):
    return "ìŒì„± ì¸ì‹ ê¸°ëŠ¥ì€ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

def query_groq_llm(prompt, api_key):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "mixtral-8x7b-32768",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 200
    }

    try:
        res = requests.post(url, headers=headers, json=payload)

        print("ğŸ” ì‘ë‹µ ìƒíƒœ ì½”ë“œ:", res.status_code)
        print("ğŸ” ì‘ë‹µ ë³¸ë¬¸ (text):", res.text)

        # ì‘ë‹µì´ JSONì´ ì•„ë‹ ìˆ˜ë„ ìˆì–´ì„œ ë¨¼ì € ì²´í¬
        try:
            response_json = res.json()
            print("ğŸ” ì‘ë‹µ JSON ì „ì²´:", response_json)
        except Exception as parse_error:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", parse_error)
            return "[ì˜¤ë¥˜] ì‘ë‹µì„ JSONìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì •ìƒ ì‘ë‹µì´ë©´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
        if res.status_code == 200:
            choice = response_json.get("choices", [{}])[0]
            message = choice.get("message", {})
            if "content" in message:
                return message["content"]
            elif "text" in choice:
                return choice["text"]
            else:
                return "[ì˜¤ë¥˜] ì‘ë‹µì— textë‚˜ contentê°€ ì—†ìŒ"
        else:
            return f"[ì˜¤ë¥˜] ìƒíƒœ ì½”ë“œ {res.status_code} - {response_json}"

    except Exception as e:
        print("âŒ ìš”ì²­ ì‹¤íŒ¨:", e)


def multimodal_chat(user_text, user_image_path, user_audio_path, api_key):
    context = []

    if user_image_path:
        caption = generate_image_caption(user_image_path, api_key)
        context.append(f"ì´ë¯¸ì§€ ì„¤ëª…: {caption}")

    if user_audio_path:
        spoken_text = speech_to_text(user_audio_path, api_key)
        context.append(f"ìŒì„± ë‚´ìš©: {spoken_text}")

    if user_text.strip():
        context.append(f"ì‚¬ìš©ì ì…ë ¥: {user_text}")